{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Pipeline SDK Demo\n",
    "\n",
    "This notebook is based on [Kubeflow doc](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/build-pipeline/).\n",
    "We will create a pipeline YAML specification via python function-based method with the KFP SDK v1.\n",
    "\n",
    "First let's install the kfp sdk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==1.8.22 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function’s arguments are decorated with the `kfp.components.InputPath` and the `kfp.components.OutputPath` annotations. These annotations let Kubeflow Pipelines know to provide the path to the zipped tar file and to create a path where your function stores the merged CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(file_path: comp.InputPath('Tarball'),\n",
    "              output_csv: comp.OutputPath('CSV')):\n",
    "  import glob\n",
    "  import pandas as pd\n",
    "  import tarfile\n",
    "\n",
    "  tarfile.open(name=file_path, mode=\"r|gz\").extractall('data')\n",
    "  df = pd.concat(\n",
    "      [pd.read_csv(csv_file, header=None) \n",
    "       for csv_file in glob.glob('data/*.csv')])\n",
    "  df.to_csv(output_csv, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `kfp.components.create_component_from_func` to return a factory function that you can use to create pipeline steps. This example also specifies the base container image to run this function in, the path to save the component specification to, and a list of PyPI packages that need to be installed in the container at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_merge_csv = kfp.components.create_component_from_func(\n",
    "    func=merge_csv,\n",
    "    output_component_file='tmp/component.yaml', # This is optional. It saves the component spec for future use.\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `kfp.components.load_component_from_url` to load the component specification YAML for any components that you are reusing in this pipeline.\n",
    "\n",
    "\n",
    "Define your pipeline as a Python function.\n",
    "\n",
    "Your pipeline function’s arguments define your pipeline’s parameters. Use pipeline parameters to experiment with different hyperparameters, such as the learning rate used to train a model, or pass run-level inputs, such as the path to an input file, into a pipeline run.\n",
    "\n",
    "Use the factory functions created by `kfp.components.create_component_from_func` and `kfp.components.load_component_from_url` to create your pipeline’s tasks. The inputs to the component factory functions can be pipeline parameters, the outputs of other tasks, or a constant value. In this case, the web_downloader_task task uses the url pipeline parameter, and the merge_csv_task uses the data output of the web_downloader_task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_downloader_op = kfp.components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml')\n",
    "\n",
    "# Define a pipeline and create a task from a component:\n",
    "def my_pipeline(url):\n",
    "  web_downloader_task = web_downloader_op(url=url)\n",
    "  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])\n",
    "  # The outputs of the merge_csv_task can be referenced using the\n",
    "  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to compile your pipeline and save it as `pipeline.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='tmp/pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload it to pipeline via UI.\n",
    "Then you can execute a run (under an experiment) with this example csv `https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Additional method: If you setup the client already, you can upload the pipeline via SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = kfp.Client() # change arguments accordingly\n",
    "# client.create_run_from_pipeline_func(\n",
    "#     my_pipeline,\n",
    "#     arguments={\n",
    "#         'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
    "#     })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
